{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b02d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpper\\anaconda32\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jpper\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jpper\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "# Download the NLTK stop words and punkt tokenizer if not already downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    stop_words = []\n",
    "    for x in range(200):\n",
    "        stop_words.append('Footnote'+str(x))\n",
    "    words = word_tokenize(sentence)\n",
    "    #escape_words = [\"\\\\u20\", \"u20\", \"//u20\", \"/u20\"]\n",
    "    for word in words:\n",
    "        word = re.sub(r'[“”«»]', '', word)\n",
    "    filtered_sentence = [word for word in words if word.lower() not in stop_words and \"20\" not in word]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "\n",
    "def convert_to_jsonl(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            sentence = line.strip()\n",
    "            filtered_sentence = remove_stop_words(sentence)\n",
    "            if len(filtered_sentence) == 0:\n",
    "                continue  \n",
    "            decoded_text = codecs.decode(filtered_sentence, 'unicode_escape')\n",
    "            # Remove footnotes (e.g., Footnote3, Footnote4)\n",
    "            cleaned_text = re.sub(r'Footnote\\d+', '', decoded_text)\n",
    "            filtered_sentence = cleaned_text\n",
    "            # Remove footnotes (e.g., Footnote3, Footnote4)\n",
    "            cleaned_text = re.sub(r'Footnote\\d+', '', cleaned_text)\n",
    "            json_line = json.dumps({\"prompt\": \"\", \"sentence\": filtered_sentence})\n",
    "            outfile.write(json_line + '\\n')\n",
    "\n",
    "# Replace 'input.txt' with the path to your input text file\n",
    "# Replace 'output.jsonl' with the desired output JSONL file path\n",
    "#convert_to_jsonl('politic.txt', 'output.jsonl')\n",
    "#convert_to_jsonl('politic.txt', 'output.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "556f918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    # Use nltk's sentence tokenizer\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_into_groups_of_three_sentences(text):\n",
    "    # Use nltk's sentence tokenizer\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Split sentences into groups of three\n",
    "    groups_of_three = [sentences[i:i+3] for i in range(0, len(sentences), 3)]\n",
    "    \n",
    "    return groups_of_three\n",
    "    \n",
    "#def convert_to_jsonl(input_file, output_file):\n",
    " #   with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "  #      for line in infile:\n",
    "   #         sentence = line.strip()\n",
    "    #        filtered_sentence = remove_stop_words(sentence)\n",
    "     #       if len(filtered_sentence) == 0:\n",
    "      #          continue  \n",
    "       #     cleaned_text = filtered_sentence.encode('utf-8').decode('unicode_escape')\n",
    "        #    filtered_sentence = cleaned_text\n",
    "         #   # Remove footnotes (e.g., Footnote3, Footnote4)\n",
    "          #  cleaned_text = re.sub(r'Footnote\\d+', '', cleaned_text)\n",
    "           # json_line = json.dumps({\"prompt\": \"\", \"sentence\": filtered_sentence})\n",
    "            #outfile.write(json_line + '\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98a0274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import unicodedata\n",
    "\n",
    "def txt_to_str(input_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        file_content = infile.read()\n",
    "        return file_content\n",
    "\n",
    "def sub_stop_words():\n",
    "    file = txt_to_str('politic.txt')\n",
    "    #file = txt_to_str('cleanedsecondbook.txt')\n",
    "    filtered_sentence = remove_stop_words(file)\n",
    "    #post footnote sub\n",
    "    cleaned_text = clean_text(filtered_sentence)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Decode Unicode escape sequences\n",
    "    text = re.sub(r'\\\\u[0-9a-fA-F]{4}', lambda x: chr(int(x.group(0)[2:], 16)), text)\n",
    "\n",
    "    # Remove footnotes (e.g., Footnote3, Footnote4)\n",
    "    cleaned_text = re.sub(r'Footnote\\d+', '', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    chunks = re.split(r'\\. ', text)\n",
    "\n",
    "    # Combine every 4 chunks into a new list\n",
    "    grouped_chunks = ['. '.join(chunks[i:i+chunk_size]) for i in range(0, len(chunks), chunk_size)]\n",
    "    \n",
    "    return grouped_chunks\n",
    "\n",
    "#deprecated\n",
    "def write_chunks_to_jsonx(chunks, output_file):\n",
    "    message = \"You are an expert in political science theory, particularly constructivism. You should help the user by giving your \"\n",
    "    with open(output_file, 'w') as file:\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            content = \"\"\n",
    "            for x in chunk:\n",
    "                content += x\n",
    "            json_object = {'prompt': 'constructivist ideas', 'completion': content}\n",
    "            json.dump(json_object, file)\n",
    "            file.write('\\n')\n",
    "            \n",
    "def write_chunks_to_json(chunks, output_file):\n",
    "    message = \"You are an expert in political science theory, particularly constructivism. You should help the user by giving your constructivist opinion related to the provided context.\"\n",
    "    question = \"\"\n",
    "    with open(output_file, 'w') as file:\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            #for x in chunk: \n",
    "             #   x.replace('\\u2019', \"''\")\n",
    "                \n",
    "            #chunk = chunk[0]    \n",
    "            content = \"\"\n",
    "            \n",
    "            for x in chunk:\n",
    "                content += x\n",
    "            \n",
    "            json_object = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": message},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "                {\"role\": \"assistant\", \"content\": content},\n",
    "                ]\n",
    "            }\n",
    "            #json_line = json.dumps({\"prompt\": \"\", \"sentence\": content})\n",
    "            #modified_json = json_object.replace('\\u2019', \"'\")\n",
    "            json.dump(json_object, file)\n",
    "            file.write('\\n')\n",
    "            \n",
    "            \n",
    "#book = sub_stop_words()\n",
    "#book = citations_remover(book)\n",
    "#split_book = split_into_chunks(book, 3)\n",
    "#write_chunks_to_json(split_book, '2train21.jsonl')\n",
    "#write_chunks_to_json(split_book, 'FirstBookRedux.json')\n",
    "#write_chunks_to_json(split_book, 'FirstBookRedux2.jsonl')\n",
    "\n",
    "#gpt training\n",
    "\n",
    "#file_path = 'newoutput.txt'\n",
    "\n",
    "# Write the string to the text file\n",
    "#with open(file_path, 'w', encoding='utf-8') as file:\n",
    " #   file.write(sub_stop_words())\n",
    "\n",
    "    \n",
    "#print(sub_stop_words()[0:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6f96b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celebrated by some and dismissed or even regarded as dangerous by others , constructivism has acquired considerable significance in International Relations. Whilst judgements on the value and validity of constructivism differ widely , it seems all but impossible not to have an opinion. As a consequence , it is important to provide a critique that engages the claims of constructivism in detail. However , there is debate not only about whether constructivism is good for us but also , given the intellectual diversity of work labelled constructivist , about what it is in the first place\n"
     ]
    }
   ],
   "source": [
    "#some more second book processing \n",
    "def citations_remover(sentence):\n",
    "    sentence = re.sub(r\"\\s\\([A-Z][a-z]+,\\s[A-Z][a-z]?\\.[^\\)]*,\\s\\d{4}\\)\", \"\", sentence)\n",
    "    sentence = re.sub(r'\\d', '', sentence)\n",
    "    sentence = sentence.replace(\"- \", \"\")\n",
    "    sentence = sentence.replace(\" . \", \". \")\n",
    "    return sentence\n",
    "\n",
    "blank_text = \"Celebrated by some and dismissed or even regarded as dangerous by others , constructivism has acquired considerable significance in Inter- national Relations.1 Whilst judgements on the value and validity of constructivism differ widely , it seems all but impossible not to have an opinion . As a consequence , it is important to provide a critique that engages the claims of constructivism in detail . However , there is debate not only about whether constructivism is good for us but also , given the intellectual diversity of work labelled constructivist , about what it is in the first place\"\n",
    "print(citations_remover(blank_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf0ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"FirstBookRedux2.jsonl\"\n",
    "#output_file_path = \"trainafteradj.jsonl\"\n",
    "output_file_path = \"FirstBookReduxActual.jsonl\"\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "# Process each line (assuming each line is a valid JSON string)\n",
    "modified_lines = []\n",
    "for line in lines:\n",
    "    # Load the JSON string into a dictionary\n",
    "    json_object = json.loads(line)\n",
    "    \n",
    "    # Check if the \"assistant\" role exists in the current line\n",
    "    if \"assistant\" in json_object[\"messages\"][-1][\"role\"]:\n",
    "        # Replace the Unicode representation in the content\n",
    "        json_object[\"messages\"][-1][\"content\"] = json_object[\"messages\"][-1][\"content\"].replace('\\u2019', \"'\")\n",
    "    \n",
    "    # Convert the modified dictionary back to a JSON string with newline\n",
    "    modified_line = json.dumps(json_object, ensure_ascii=False) + '\\n'\n",
    "    \n",
    "    # Append the modified line to the list\n",
    "    modified_lines.append(modified_line)\n",
    "\n",
    "# Write the modified lines to the output file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    outfile.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0c0f13",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'completion'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12356\\1878329695.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Check if certain conditions are met to modify the \"answer\" field\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Replace specific content in the \"answer\" field\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mjson_object\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"completion\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_object\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"completion\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\u2018'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\u2019'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Convert the modified dictionary back to a JSON string with newline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'completion'"
     ]
    }
   ],
   "source": [
    "input_file_path = \"FirstBookRedux2.jsonl\"\n",
    "#output_file_path = \"trainafteradj.jsonl\"\n",
    "output_file_path = \"FirstBookReduxActual.jsonl\"\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "# Process each line (assuming each line is a valid JSON string)\n",
    "modified_lines = []\n",
    "for line in lines:\n",
    "    # Load the JSON string into a dictionary\n",
    "    json_object = json.loads(line)\n",
    "    \n",
    "    # Check if certain conditions are met to modify the \"answer\" field\n",
    "        # Replace specific content in the \"answer\" field\n",
    "    json_object[\"completion\"] = json_object[\"completion\"].replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    \n",
    "    # Convert the modified dictionary back to a JSON string with newline\n",
    "    modified_line = json.dumps(json_object, ensure_ascii=False) + '\\n'\n",
    "    \n",
    "    # Append the modified line to the list\n",
    "    modified_lines.append(modified_line)\n",
    "\n",
    "# Write the modified lines to the output file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    outfile.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f609c3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 3733: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9600\\1656199276.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'fulltrain.jsonl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mcombine_jsonl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile1_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile2_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9600\\1656199276.py\u001b[0m in \u001b[0;36mcombine_jsonl\u001b[1;34m(file1_path, file2_path, output_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Read and combine data from the first file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile1_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mcombined_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 3733: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "#combining both files\n",
    "\n",
    "import json\n",
    "\n",
    "def combine_jsonl(file1_path, file2_path, output_path):\n",
    "    combined_data = []\n",
    "\n",
    "    # Read and combine data from the first file\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        for line in file1:\n",
    "            combined_data.append(json.loads(line))\n",
    "\n",
    "    # Read and combine data from the second file\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        for line in file2:\n",
    "            combined_data.append(json.loads(line))\n",
    "\n",
    "    # Write combined data to a new file\n",
    "    with open(output_path, 'w') as output_file:\n",
    "        for data_point in combined_data:\n",
    "            output_file.write(json.dumps(data_point) + '\\n')\n",
    "\n",
    "# Example usage\n",
    "file1_path = '1trainafteradj.jsonl'\n",
    "file2_path = '2trainafteradj.jsonl'\n",
    "output_path = 'fulltrain.jsonl'\n",
    "\n",
    "combine_jsonl(file1_path, file2_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "242adc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def write_chunks_to_df(chunks, output_file):\n",
    "    data = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        content = \"\"\n",
    "        for x in chunk:\n",
    "            content += x\n",
    "        \n",
    "        json_object = {\n",
    "            \"content\": content\n",
    "        }\n",
    "        data.append(json_object)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "# Write DataFrame to a CSV file\n",
    "    df.to_csv(output_file, index=False)    \n",
    "            \n",
    "book = sub_stop_words()\n",
    "book = citations_remover(book)\n",
    "split_book = split_into_chunks(book, 3)\n",
    "#write_chunks_to_json(split_book, '2train21.jsonl')\n",
    "#write_chunks_to_json(split_book, 'FirstBookRedux.json')\n",
    "write_chunks_to_df(split_book, 'FirstBook.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98059656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
